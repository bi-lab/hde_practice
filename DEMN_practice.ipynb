{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep-Embedded-Memory-Networks (Keras version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authors: Kyung-Min Kim, Min-Oh Heo, Seong-Ho Choi, and Byoung-Tak Zhang (Seoul National University & Surromind Robotics)\n",
    "#### Paper: DeepStory: Video Story QA by Deep Embedded Memory Networks (https://arxiv.org/abs/1707.00836) (IJCAI 2017)\n",
    "This notebook shows how the DEMN works. The DEMN consists of three modules (video story understanding, story selection, answer selection). This code corresponds to QA modules (story selection, answer selection) among them.\n",
    "\n",
    "PororoQA dataset release: https://github.com/Kyung-Min/PororoQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Kyung-Min/Deep-Embedded-Memory-Networks\n",
    "#\n",
    "#References:\n",
    "#- Kyung-Min Kim, Min-Oh Heo, Seong-Ho Choi, and Byoung-Tak Zhang,\n",
    "#  \"DeepStory: Video Story QA by Deep Embedded Memory Networks\",\n",
    "#  https://arxiv.org/abs/1707.00836 (IJCAI 2017)\n",
    "\n",
    "# glove 300k (wget http://nlp.stanford.edu/data/glove.6B.zip | tar \n",
    "\n",
    "#%env CUDA_VISIBLE_DEVICES=1\n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import utils\n",
    "\n",
    "import keras.activations as activations\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Input, TimeDistributed\n",
    "from keras.layers.merge import concatenate, add, multiply\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.layers.core import Activation, Dense, Dropout, Flatten, Lambda, Permute, RepeatVector\n",
    "from keras.layers.recurrent import GRU, LSTM\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "import csv\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config():\n",
    "    c = dict()\n",
    "    # embedding params\n",
    "    c['emb'] = 'Glove'\n",
    "    c['embdim'] = 300\n",
    "    c['inp_e_dropout'] = 1/2\n",
    "\n",
    "    # objective function\n",
    "    c['loss'] = 'ranking_loss'  \n",
    "    c['margin'] = 1\n",
    "\n",
    "    # training hyperparams\n",
    "    c['opt'] = 'adam'\n",
    "    c['batch_size'] = 160   \n",
    "    c['epochs'] = 16\n",
    "    \n",
    "    # sentences with word lengths below the 'pad' will be padded with 0.\n",
    "    c['pad'] = 60\n",
    "    \n",
    "    # scoring function: word-level attention-based model\n",
    "    c['dropout'] = 1/2     \n",
    "    c['dropoutfix_inp'] = 0\n",
    "    c['dropoutfix_rec'] = 0           \n",
    "    c['l2reg'] = 1e-4\n",
    "                                              \n",
    "    c['rnnbidi'] = True                      \n",
    "    c['rnn'] = GRU                                                     \n",
    "    c['rnnbidi_mode'] = add\n",
    "    c['rnnact'] = 'tanh'\n",
    "    c['rnninit'] = 'glorot_uniform'                      \n",
    "    c['sdim'] = 1\n",
    "\n",
    "    c['pool_layer'] = MaxPooling1D\n",
    "    c['cnnact'] = 'tanh'\n",
    "    c['cnninit'] = 'glorot_uniform'\n",
    "    c['cdim'] = 2\n",
    "    c['cfiltlen'] = 3\n",
    "    \n",
    "    c['adim'] = 1/2\n",
    "\n",
    "    # mlp scoring function\n",
    "    c['Ddim'] = 2\n",
    "    \n",
    "    ps, h = utils.hash_params(c)\n",
    "\n",
    "    return c, ps, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = None\n",
    "emb = None\n",
    "vocab = None\n",
    "inp_tr = None\n",
    "inp_val = None\n",
    "inp_test = None\n",
    "y_val = None\n",
    "y_test = None\n",
    "\n",
    "params = []\n",
    "\n",
    "conf, ps, h = config()\n",
    "\n",
    "if conf['emb'] == 'Glove':\n",
    "    print('GloVe')\n",
    "    emb = utils.GloVe(N=conf['embdim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_file(dsfile, iseval):     #load a dataset in the csv format;\n",
    "    q = [] # a set of questions\n",
    "    s_p = [] # if training time, s1 is a set of positive sentences. Otherwise, s1 is a set of sentences.\n",
    "    s_n = [] # if training time, s2 is a set of negative sentences. Otherwise, s2 is a set of dummy sentences.\n",
    "    q_sp = [] # a set of sentences which concatenate questions and positive sentences\n",
    "    a_p = [] # a set of positive answers\n",
    "    a_n = [] # a set of negative answers\n",
    "    labels = []\n",
    "\n",
    "    with open(dsfile) as f:\n",
    "        c = csv.DictReader(f)\n",
    "        for l in c:\n",
    "            if iseval:\n",
    "                label = int(l['label'])\n",
    "                labels.append(label)\n",
    "            try:\n",
    "                qtext = l['qtext'].decode('utf8')\n",
    "                s_p_text = l['atext1'].decode('utf8')\n",
    "                s_n_text = l['atext2'].decode('utf8')\n",
    "            except AttributeError:  # python3 has no .decode()\n",
    "                qtext = l['qtext']\n",
    "                s_p_text = l['atext1']\n",
    "                s_n_text = l['atext2']\n",
    "            a_p_text = l['a1'].decode('utf8')\n",
    "            a_n_text = l['a2'].decode('utf8')\n",
    "            a_p.append(a_p_text.split(' '))\n",
    "            a_n.append(a_n_text.split(' '))\n",
    "            \n",
    "            q.append(qtext.split(' '))\n",
    "            s_p.append(s_p_text.split(' '))\n",
    "            s_n.append(s_n_text.split(' '))\n",
    "            q_sp.append(qtext.split(' ')+s_p_text.split(' '))\n",
    "    if iseval:\n",
    "        return (q, s_p, s_n, q_sp, a_p, a_n, np.array(labels))\n",
    "    else:\n",
    "        return (q, s_p, s_n, q_sp, a_p, a_n)\n",
    "    \n",
    "def make_model_inputs(qi, si_p, si_n, qi_si, ai_p, ai_n, f01, f10, f02, f20, f31, f13, f32, f23, \n",
    "                      q, s_p, s_n, q_sp, a_p, a_n, y=None):\n",
    "    inp = {'qi': qi, 'si_p': si_p, 'si_n': si_n, 'qi_si':qi_si, 'ai_p':ai_p, \n",
    "          'ai_n':ai_n, 'f01':f01, 'f10':f10, 'f02':f02, 'f20':f20, 'f31':f31, \n",
    "          'f13':f13, 'f32':f32, 'f23':f23, 'q':q, 's_p':s_n, 's_n':s_n, 'q_sp':q_sp, 'a_p':a_p, 'a_n':a_n} \n",
    "    \n",
    "    if y is not None:\n",
    "        inp['y'] = y\n",
    "    return inp\n",
    "\n",
    "def load_set(fname, vocab=None, iseval=False):\n",
    "    if iseval:\n",
    "        q, s_p, s_n, q_sp, a_p, a_n, y = load_data_from_file(fname, iseval)\n",
    "    else:\n",
    "        q, s_p, s_n, q_sp, a_p, a_n = load_data_from_file(fname, iseval)\n",
    "        vocab = utils.Vocabulary(q + s_p + s_n + a_p + a_n) \n",
    "    \n",
    "    pad = conf['pad']\n",
    "    \n",
    "    qi = vocab.vectorize(q, pad=pad)  \n",
    "    si_p = vocab.vectorize(s_p, pad=pad)\n",
    "    si_n = vocab.vectorize(s_n, pad=pad)\n",
    "    qi_si = vocab.vectorize(q_sp, pad=pad)\n",
    "    ai_p = vocab.vectorize(a_p, pad=pad)\n",
    "    ai_n = vocab.vectorize(a_n, pad=pad)\n",
    "    \n",
    "    f01, f10 = utils.sentence_flags(q, s_p, pad)  \n",
    "    f02, f20 = utils.sentence_flags(q, s_n, pad)\n",
    "    f31, f13 = utils.sentence_flags(q_sp, a_p, pad)\n",
    "    f32, f23 = utils.sentence_flags(q_sp, a_n, pad)\n",
    "    if iseval:\n",
    "        inp = make_model_inputs(qi, si_p, si_n, qi_si, ai_p, ai_n, f01, f10, f02, f20, \n",
    "                                f31, f13, f32, f23, q, s_p, s_n, q_sp, a_p, a_n, y=y)\n",
    "        return (inp, y)\n",
    "    else:\n",
    "        inp = make_model_inputs(qi, si_p, si_n, qi_si, ai_p, ai_n, f01, f10, f02, f20, \n",
    "                            f31, f13, f32, f23, q, s_p, s_n, q_sp, a_p, a_n)\n",
    "        return (inp, vocab)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--- Dataset ---')\n",
    "trainf = '/root/data/hde_practice/pororoQA_train_500.csv' \n",
    "valf = '/root/data/hde_practice/pororoQA_test_500.csv'\n",
    "testf = '/root/data/hde_practice/pororoQA_dev_500.csv'\n",
    "\n",
    "inp_tr, vocab = load_set(trainf, iseval=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(inp_tr)\n",
    "inp_tr.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data check\n",
    "sh_idx=900\n",
    "print('Q:   ' + ' '.join(inp_tr['q'][sh_idx]))\n",
    "print('stn_p: '+' '.join(inp_tr['s_p'][sh_idx]))\n",
    "print('stn_n: '+' '.join(inp_tr['s_n'][sh_idx]))\n",
    "print('ans_p: '+' '.join(inp_tr['a_p'][sh_idx]))\n",
    "print('ans_n: '+' '.join(inp_tr['a_n'][sh_idx]))\n",
    "#print('q_sp:  '+' '.join(inp_tr['q_sp'][sh_idx]))\n",
    "\n",
    "print('Q (i): ' + str(inp_tr['qi'][sh_idx]))\n",
    "print('stn_p (i): ' + str(inp_tr['si_p'][sh_idx]))\n",
    "print('stn_n (i): '+ str(inp_tr['si_n'][sh_idx]))\n",
    "print('ans_p (i): ' + str(inp_tr['ai_p'][sh_idx]))\n",
    "print('ans_n (i): '+ str(inp_tr['ai_n'][sh_idx]))\n",
    "#print('qi_si (i): '+ str(inp_tr['qi_si'][sh_idx]))\n",
    "\n",
    "#print('flag 01:' + str(inp_tr['f01'][sh_idx]))\n",
    "#print('flag 02:' + str(inp_tr['f02'][sh_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils.sentence_flags(['33 Bears Live in The forest.'], ['Bears forest.'], conf['pad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load validation dataset and test dataset\n",
    "\n",
    "inp_val, y_val = load_set(valf, vocab=vocab, iseval=True)\n",
    "inp_test, y_test = load_set(testf, vocab=vocab, iseval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding():\n",
    "    '''\n",
    "    Declare all inputs (vectorized sentences and NLP flags)\n",
    "    and generate outputs representing vector sequences with dropout applied.  \n",
    "    Returns the vector dimensionality.       \n",
    "    '''\n",
    "    pad = conf['pad']\n",
    "    dropout = conf['inp_e_dropout']\n",
    "    \n",
    "    # story selection\n",
    "    input_qi = Input(name='qi', shape=(pad,), dtype='int32')                          \n",
    "    input_si_p = Input(name='si_p', shape=(pad,), dtype='int32')                 \n",
    "    input_f01 = Input(name='f01', shape=(pad, utils.flagsdim))\n",
    "    input_f10 = Input(name='f10', shape=(pad, utils.flagsdim))\n",
    "\n",
    "    input_si_n = Input(name='si_n', shape=(pad,), dtype='int32')  \n",
    "    input_f02 = Input(name='f02', shape=(pad, utils.flagsdim))\n",
    "    input_f20 = Input(name='f20', shape=(pad, utils.flagsdim))             \n",
    "\n",
    "    # answer selection\n",
    "    input_qi_si = Input(name='qi_si', shape=(pad,), dtype='int32')\n",
    "    input_ai_p = Input(name='ai_p', shape=(pad,), dtype='int32')                        \n",
    "    input_f31 = Input(name='f31', shape=(pad, utils.flagsdim))              \n",
    "    input_f13 = Input(name='f13', shape=(pad, utils.flagsdim))          \n",
    "\n",
    "    input_ai_n = Input(name='ai_n', shape=(pad,), dtype='int32')         \n",
    "    input_f32 = Input(name='f32', shape=(pad, utils.flagsdim))            \n",
    "    input_f23 = Input(name='f23', shape=(pad, utils.flagsdim))                       \n",
    "\n",
    "    input_nodes = [input_qi, input_si_p, input_f01, input_f10, input_si_n,         \n",
    "            input_f02, input_f20, input_qi_si, input_ai_p, input_f31, input_f13,\n",
    "            input_ai_n, input_f32, input_f23]           \n",
    "        \n",
    "    N = emb.N + utils.flagsdim\n",
    "    shared_embedding = Embedding(name='emb', input_dim=vocab.size(), input_length=pad,\n",
    "                                output_dim=emb.N, mask_zero=False,\n",
    "                                weights=[vocab.embmatrix(emb)], trainable=True)\n",
    "    emb_qi_p = Dropout(dropout, noise_shape=(N,))(concatenate([shared_embedding(input_qi),\n",
    "        input_f01]))\n",
    "    emb_si_p = Dropout(dropout, noise_shape=(N,))(concatenate([shared_embedding(input_si_p),\n",
    "        input_f10]))\n",
    "    emb_qi_n = Dropout(dropout, noise_shape=(N,))(concatenate([shared_embedding(input_qi),\n",
    "        input_f02]))\n",
    "    emb_si_n = Dropout(dropout, noise_shape=(N,))(concatenate([shared_embedding(input_si_n),\n",
    "        input_f20]))\n",
    "    emb_qi_si_p = Dropout(dropout, noise_shape=(N,))(concatenate([shared_embedding(input_qi_si),\n",
    "        input_f31]))\n",
    "    emb_ai_p = Dropout(dropout, noise_shape=(N,))(concatenate([shared_embedding(input_ai_p),\n",
    "        input_f13]))\n",
    "    emb_qi_si_n = Dropout(dropout, noise_shape=(N,))(concatenate([shared_embedding(input_qi_si),\n",
    "        input_f32]))\n",
    "    emb_ai_n = Dropout(dropout, noise_shape=(N,))(concatenate([shared_embedding(input_ai_n),\n",
    "        input_f23]))\n",
    "\n",
    "    emb_outputs = [emb_qi_p, emb_si_p, emb_qi_n, emb_si_n, emb_qi_si_p, emb_ai_p, emb_qi_si_n, emb_ai_n]\n",
    "    \n",
    "    return N, input_nodes, emb_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_model(input_nodes, N, pfx=''):\n",
    "    # apply biLSTM on each sentence X,Y\n",
    "    qpos, pos, qneg, neg = rnn_input(N, pfx=pfx, dropout=conf['dropout'], dropoutfix_inp=conf['dropoutfix_inp'], \n",
    "                            dropoutfix_rec=conf['dropoutfix_rec'], sdim=conf['sdim'], \n",
    "                            rnnbidi_mode=conf['rnnbidi_mode'], rnn=conf['rnn'], rnnact=conf['rnnact'], \n",
    "                            rnninit=conf['rnninit'], inputs=input_nodes)\n",
    "    \n",
    "    # calculate the sentence vector on X side using Convolutional Neural Networks\n",
    "    qpos_aggreg, qneg_aggreg, gwidth = aggregate(qpos, qneg, 'aggre_q'+pfx, N, \n",
    "                                               dropout=conf['dropout'], l2reg=conf['l2reg'], \n",
    "                                               sdim=conf['sdim'], cnnact=conf['cnnact'], cdim=conf['cdim'], \n",
    "                                               cfiltlen=conf['cfiltlen'])\n",
    "    \n",
    "    # re-embed X,Y in attention space\n",
    "    awidth = int(N*conf['adim'])\n",
    "    \n",
    "    shared_dense_q = Dense(awidth, name='attn_proj_q'+pfx, kernel_regularizer=l2(conf['l2reg']))\n",
    "    qpos_aggreg_attn = shared_dense_q(qpos_aggreg)\n",
    "    qneg_aggreg_attn = shared_dense_q(qneg_aggreg)\n",
    "    \n",
    "    shared_dense_s = Dense(awidth, name='attn_proj_s'+pfx, kernel_regularizer=l2(conf['l2reg']))\n",
    "    pos_attn = TimeDistributed(shared_dense_s)(pos)\n",
    "    neg_attn = TimeDistributed(shared_dense_s)(neg)\n",
    "    \n",
    "    # apply an attention function on Y side by producing an vector of scalars denoting the attention for each token\n",
    "    pos_foc, neg_foc = focus(N, qpos_aggreg_attn, qneg_aggreg_attn, pos_attn, neg_attn, \n",
    "                             pos, neg, conf['sdim'], awidth, \n",
    "                             conf['l2reg'], pfx=pfx)\n",
    "\n",
    "    # calculate the sentence vector on Y side using Convolutional Neural Networks\n",
    "    pos_aggreg, neg_aggreg, gwidth = aggregate(pos_foc, neg_foc, 'aggre_s'+pfx, N, \n",
    "                                  dropout=conf['dropout'], l2reg=conf['l2reg'], sdim=conf['sdim'],\n",
    "                                  cnnact=conf['cnnact'], cdim=conf['cdim'], cfiltlen=conf['cfiltlen'])\n",
    "\n",
    "    return ([qpos_aggreg, pos_aggreg], [qneg_aggreg, neg_aggreg]) \n",
    "    \n",
    "def rnn_input(N, dropout=3/4, dropoutfix_inp=0, dropoutfix_rec=0,           \n",
    "              sdim=2, rnn=GRU, rnnact='tanh', rnninit='glorot_uniform', rnnbidi_mode=add, \n",
    "              inputs=None, pfx=''):\n",
    "    if rnnbidi_mode == 'concat':\n",
    "        sdim /= 2\n",
    "    shared_rnn_f = rnn(int(N*sdim), kernel_initializer=rnninit, input_shape=(None, conf['pad'], N), \n",
    "                       activation=rnnact, return_sequences=True, dropout=dropoutfix_inp,\n",
    "                       recurrent_dropout=dropoutfix_rec, name='rnnf'+pfx)\n",
    "    shared_rnn_b = rnn(int(N*sdim), kernel_initializer=rnninit, input_shape=(None, conf['pad'], N),\n",
    "                       activation=rnnact, return_sequences=True, dropout=dropoutfix_inp,\n",
    "                       recurrent_dropout=dropoutfix_rec, go_backwards=True, name='rnnb'+pfx)\n",
    "    qpos_f = shared_rnn_f(inputs[0])\n",
    "    pos_f = shared_rnn_f(inputs[1])\n",
    "    qneg_f = shared_rnn_f(inputs[2])\n",
    "    neg_f = shared_rnn_f(inputs[3])\n",
    "    \n",
    "    qpos_b = shared_rnn_b(inputs[0])\n",
    "    pos_b = shared_rnn_b(inputs[1])\n",
    "    qneg_b = shared_rnn_b(inputs[2])\n",
    "    neg_b = shared_rnn_b(inputs[3])\n",
    "\n",
    "    qpos = Dropout(dropout, noise_shape=(conf['pad'], int(N*sdim)))(rnnbidi_mode([qpos_f, qpos_b]))\n",
    "    pos = Dropout(dropout, noise_shape=(conf['pad'], int(N*sdim)))(rnnbidi_mode([pos_f, pos_b]))\n",
    "    qneg = Dropout(dropout, noise_shape=(conf['pad'], int(N*sdim)))(rnnbidi_mode([qneg_f, qneg_b]))\n",
    "    neg = Dropout(dropout, noise_shape=(conf['pad'], int(N*sdim)))(rnnbidi_mode([neg_f, neg_b]))\n",
    "    \n",
    "    return (qpos, pos, qneg, neg)\n",
    "\n",
    "def aggregate(in1, in2, pfx, N, dropout, l2reg, sdim, cnnact, cdim, cfiltlen):\n",
    "    '''\n",
    "    In the paper, the sentence vector was calculated using simple averaging, \n",
    "    but we will use Convolutional Neural Networks in the demo.\n",
    "    '''\n",
    "    \n",
    "    shared_conv = Convolution1D(name=pfx+'c', input_shape=(conf['pad'], int(N*sdim)), kernel_size=cfiltlen, \n",
    "                                filters=int(N*cdim), activation=cnnact, kernel_regularizer=l2(l2reg))\n",
    "    aggreg1 = shared_conv(in1)\n",
    "    aggreg2 = shared_conv(in2)\n",
    "\n",
    "    nsteps = conf['pad'] - cfiltlen + 1\n",
    "    width = int(N*cdim)\n",
    "    \n",
    "    aggreg1, aggreg2 = pool(pfx, aggreg1, aggreg2, nsteps, width, dropout=dropout)\n",
    "    \n",
    "    return (aggreg1, aggreg2, width)\n",
    "\n",
    "def pool(pfx, in1, in2, nsteps, width, dropout):\n",
    "    pooling = MaxPooling1D(pool_size=nsteps, name=pfx+'pool0')\n",
    "    out1 = pooling(in1)\n",
    "    out2 = pooling(in2)\n",
    "    \n",
    "    flatten = Flatten(name=pfx+'pool1')\n",
    "    out1 = Dropout(dropout, noise_shape=(1, width))(flatten(out1))\n",
    "    out2 = Dropout(dropout, noise_shape=(1, width))(flatten(out2))\n",
    "    \n",
    "    return (out1, out2)\n",
    "    \n",
    "def focus(N, input_aggreg1, input_aggreg2, input_seq1, input_seq2, orig_seq1, orig_seq2,\n",
    "          sdim, awidth, l2reg, pfx=''):\n",
    "    \n",
    "    repeat_vec = RepeatVector(conf['pad'], name='input_aggreg1_rep'+pfx)\n",
    "    input_aggreg1_rep = repeat_vec(input_aggreg1)\n",
    "    input_aggreg2_rep = repeat_vec(input_aggreg2)\n",
    "    \n",
    "    attn1 = Activation('tanh')(add([input_aggreg1_rep, input_seq1]))\n",
    "    attn2 = Activation('tanh')(add([input_aggreg2_rep, input_seq2]))\n",
    "    \n",
    "    shared_dense = Dense(1, kernel_regularizer=l2(l2reg), name='focus1'+pfx)\n",
    "    attn1 = TimeDistributed(shared_dense)(attn1)\n",
    "    attn2 = TimeDistributed(shared_dense)(attn2)\n",
    "    \n",
    "    flatten = Flatten(name='attn_flatten'+pfx)\n",
    "    attn1 = flatten(attn1)\n",
    "    attn2 = flatten(attn2)\n",
    "    \n",
    "    attn1 = Activation('softmax')(attn1)\n",
    "    attn1 = RepeatVector(int(N*sdim))(attn1)\n",
    "    attn1 = Permute((2,1))(attn1)\n",
    "    output1 = multiply([orig_seq1, attn1])\n",
    "    \n",
    "    attn2 = Activation('softmax')(attn2)\n",
    "    attn2 = RepeatVector(int(N*sdim))(attn2)\n",
    "    attn2 = Permute((2,1))(attn2)\n",
    "    output2 = multiply([orig_seq2, attn2])\n",
    "    \n",
    "    return (output1, output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_ptscorer(inputs1, inputs2,  Ddim, N, l2reg, pfx='out', oact='sigmoid', extra_inp=[]):\n",
    "    \"\"\" Element-wise features from the pair fed to an MLP. \"\"\"\n",
    "\n",
    "    sum1 = add(inputs1)\n",
    "    sum2 = add(inputs2)\n",
    "    mul1 = multiply(inputs1)\n",
    "    mul2 = multiply(inputs2)\n",
    "\n",
    "    mlp_input1 = concatenate([sum1, mul1])\n",
    "    mlp_input2 = concatenate([sum2, mul2])\n",
    "\n",
    "    # Ddim may be either 0 (no hidden layer), scalar (single hidden layer) or\n",
    "    # list (multiple hidden layers)\n",
    "    if Ddim == 0:\n",
    "        Ddim = []\n",
    "    elif not isinstance(Ddim, list):\n",
    "        Ddim = [Ddim]\n",
    "    if Ddim:\n",
    "        for i, D in enumerate(Ddim):\n",
    "            shared_dense = Dense(int(N*D), kernel_regularizer=l2(l2reg), \n",
    "                                 activation='tanh', name=pfx+'hdn%d'%(i,))\n",
    "            mlp_input1 = shared_dense(mlp_input1)\n",
    "            mlp_input2 = shared_dense(mlp_input2)\n",
    "\n",
    "    shared_dense = Dense(1, kernel_regularizer=l2(l2reg), activation=oact, name=pfx+'mlp')\n",
    "    mlp_out1 = shared_dense(mlp_input1)\n",
    "    mlp_out2 = shared_dense(mlp_input2)\n",
    "    \n",
    "    return [mlp_out1, mlp_out2]    \n",
    "\n",
    "'''\n",
    "posS: G(q, s^*)\n",
    "negS: G(q, s_i)\n",
    "posA: H(s_a, s^*)\n",
    "negA: H(s_a, a_r)\n",
    "'''\n",
    "def ranking_loss(y_true, y_pred):\n",
    "    posS = y_pred[0]\n",
    "    negS = y_pred[1]\n",
    "    posA = y_pred[2]\n",
    "    negA = y_pred[3]\n",
    "\n",
    "    margin = conf['margin']\n",
    "    loss = K.maximum(margin + negS - posS, 0.0) + K.maximum(margin + negA - posA, 0.0) \n",
    "    return K.mean(loss, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_model()\n",
    "runid = 'DEMN-%x' % (h)\n",
    "print('RunID: %s  (%s)' % (runid, ps))\n",
    "print('Model')\n",
    "\n",
    "N, input_nodes_emb, output_nodes_emb = embedding()\n",
    "\n",
    "# story selection\n",
    "ptscorer_inputs1, ptscorer_inputs2 = attention_model(output_nodes_emb[:4], N, pfx='S')\n",
    "\n",
    "scoreS1, scoreS2 = mlp_ptscorer(ptscorer_inputs1, ptscorer_inputs2, conf['Ddim'], N,  \n",
    "        conf['l2reg'], pfx='outS', oact='sigmoid')                \n",
    "\n",
    "# answer selection\n",
    "ptscorer_inputs3, ptscorer_inputs4 = attention_model(output_nodes_emb[4:], N, pfx='A')\n",
    "\n",
    "scoreA1, scoreA2 = mlp_ptscorer(ptscorer_inputs3, ptscorer_inputs4, conf['Ddim'], N,\n",
    "        conf['l2reg'], pfx='outA', oact='sigmoid')\n",
    "\n",
    "output_nodes = [scoreS1, scoreS2, scoreA1, scoreA2]\n",
    "\n",
    "model = Model(inputs=input_nodes_emb, outputs=output_nodes)\n",
    "model.compile(loss=ranking_loss, optimizer=conf['opt'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_callbacks(weightsf):                                  \n",
    "    return [utils.AnsSelCB(inp_val['q'], inp_val['s_p'], inp_val['s_n'], inp_val['q_sp'], \n",
    "        inp_val['a_p'], inp_val['a_n'], y_val, inp_val),\n",
    "            ModelCheckpoint(weightsf, save_best_only=True, monitor='acc', mode='max'),\n",
    "            EarlyStopping(monitor='acc', mode='max', patience=12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightsf='weights-'+runid+'-bestval.h5'\n",
    "\n",
    "epochs = conf['epochs']\n",
    "callbacks = fit_callbacks(weightsf)\n",
    "\n",
    "# During the computation, these values will not be used at all.\n",
    "# Note that the variable 'y_true' in function ranking_loss does not participate in calculations.\n",
    "dummy1 = np.ones((len(inp_tr['qi']),1), dtype=np.float) \n",
    "dummy2 = np.ones((len(inp_val['qi']),1), dtype=np.float)\n",
    "model.fit(inp_tr, y=[dummy1,dummy1,dummy1,dummy1], validation_data=[inp_val,\n",
    "        [dummy2,dummy2,dummy2,dummy2]], callbacks = callbacks, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and load best model\n",
    "model.save_weights('weights-'+runid+'-final.h5', overwrite=True)\n",
    "model.load_weights('weights-'+runid+'-bestval.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model):\n",
    "    res = []\n",
    "    for inp in [inp_val, inp_test]:\n",
    "        if inp is None:\n",
    "            res.append(None)\n",
    "            continue\n",
    "\n",
    "        pred = model.predict(inp)\n",
    "        ypredS = pred[0]\n",
    "        ypredA1 = pred[2]\n",
    "        ypredA2 = pred[3]\n",
    "\n",
    "        res.append(utils.eval_QA(ypredS, ypredA1, ypredA2, inp['q'], inp['y'], MAP=False))\n",
    "    return tuple(res)\n",
    "\n",
    "print('Predict&Eval (best val epoch)')\n",
    "res = eval(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
